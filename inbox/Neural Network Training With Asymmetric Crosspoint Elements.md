### abstract
프로그래밍 가능한 비휘발성 저항으로 구성된 아날로그 크로스바 어레이는 심층 신경망 훈련의 가속화를 위해 집중적으로 연구되고 있습니다. 그러나 실제 저항 소자의 유비쿼터스 비대칭 전도도 변조는 기존 알고리즘으로 훈련된 네트워크의 분류 성능을 심각하게 저하시킵니다.
여기에서는 먼저 이러한 비호환성의 근본적인 이유를 설명합니다. 그런 다음 비대칭 교차점 요소와 호환되는 새로운 완전 병렬 훈련 알고리즘의 이론적 토대를 설명합니다. 고전 역학과의 강력한 비유를 통해 장치 비대칭성을 아날로그 딥 러닝 프로세서의 유용한 기능으로 활용하는 방법을 설명합니다.
기존의 오차 함수 기울기 방향으로 가중치를 조정하는 대신, 네트워크 파라미터를 프로그래밍하여 소자 비대칭의 효과를 통합한 시스템의 총 에너지(해밀턴)를 성공적으로 최소화할 수 있습니다. 이 기술을 사용하면 쉽게 구할 수 있는 장치 기술을 기반으로 아날로그 딥 러닝 가속기를 즉시 구현할 수 있습니다.

### Introduction
딥러닝은 객체 인식, 자연어 처리, 생물정보학 등의 분야에서 여러 수준의 추상화에서 데이터의 표현을 분류하고 클러스터링함으로써 이점을 얻는 패러다임의 변화를 일으켰습니다(Lecun et al., 2015). 그러나 최첨단 심층 신경망(DNN)을 훈련하기 위한 계산 워크로드는 데이터센터에 막대한 계산 시간과 에너지 비용을 요구합니다(Strubell et al., 2020).
일반적으로 더 큰 데이터 세트로 훈련된 더 큰 신경망이 더 나은 성능을 제공하기 때문에 이러한 추세는 앞으로 더욱 가속화될 것으로 예상됩니다. 그 결과, 딥러닝을 위한 빠르고 에너지 효율적인 솔루션을 제공해야 할 필요성으로 인해 업계와 학계에서 대규모 공동 연구가 진행되고 있습니다(Chen et al., 2016; Jouppi et al., 2017; Rajbhandari et al., 2020). 고도로 최적화된 디지털 애플리케이션별 집적 회로(ASIC) 구현은 계산 집약적인 행렬 연산을 위해 정밀도 감소 연산을 사용하여 DNN 워크로드를 가속화하려고 시도해 왔습니다. 추론 작업의 가속화는 2비트 해상도를 사용하여 달성되었지만(Choi et al., 2019), 학습 작업에는 최소한 하이브리드 8비트 부동 소수점 형식이 필요한 것으로 밝혀졌습니다(Sun et al., 2019). 이는 대규모 네트워크에 여전히 상당한 에너지 소비와 처리 시간을 부과합니다. 따라서 학습 워크로드를 효율적으로 처리할 수 있는 비디지털 접근 방식이 활발히 모색되고 있습니다.

아날로그 저항 크로스바 어레이를 사용한 인메모리 계산 개념은 유망한 대안으로 집중적으로 연구되고 있습니다. 이러한 프레임워크는 옴의 법칙과 키르히호프의 법칙을 사용하여 전체 계산 부하의 ≈ 2/3를 차지하는 병렬 벡터 행렬 곱셈(자세한 내용은 부록 2.1, 2.2 참조)을 수행하도록 처음 설계되었습니다(Steinbuch, 1961). 그러나 업데이트 주기 동안 나머지 ≈ 1/3의 연산도 병렬화하지 않으면 아날로그 어레이가 제공하는 가속 계수는 기존 디지털 프로세서와 비교했을 때 기껏해야 3배에 불과합니다.
나중에 펄스 일치와 디바이스 전도도의 점진적 변화를 사용하여 1등급 외부 제품도 병렬로 달성할 수 있다는 사실이 밝혀졌습니다(Burr et al., 2015; Gokmen and Vlasov, 2016). 이 방법을 사용하면 외부 곱1을 명시적으로 계산하거나 개별 크로스포인트 요소의 값을 읽을 필요 없이 전체 크로스바 어레이를 병렬로 업데이트할 수 있습니다(Gokmen et al., 2017). 그 결과, 확률적 경사 하강(SGD) 알고리즘을 사용하는 DNN 훈련의 모든 기본 요소는 아날로그 크로스바 아키텍처를 사용하여 완전 병렬 방식으로 수행될 수 있습니다.
그러나 이 병렬 업데이트 방식은 크로스포인트 요소의 컨덕턴스 변조 특성에 따라 성능이 크게 영향을 받기 때문에 엄격한 디바이스 요구 사항이 부과됩니다. 특히 비대칭 컨덕턴스 변조 특성(즉, 양의 컨덕턴스와 음의 컨덕턴스 조정이 불일치하는 경우)은 부정확한 기울기 축적을 유발하여 분류 정확도를 저하시키는 것으로 밝혀졌습니다(Yu et al., 2015; Agarwal et al., 2016, 2017; Gokmen and Vlasov, 2016; Gokmen et al., 2017, 2018; Ambrogio et al., 2018). 안타깝게도 현재까지 모든 아날로그 저항 소자는 비대칭적인 특성을 가지고 있어 아날로그 딥러닝 프로세서를 실현하는 데 큰 기술적 장벽이 되고 있습니다.

이상적인 저항 소자를 설계하려는 광범위한 노력(Woo and Yu, 2018; Fuller et al., 2019; Grollier et al., 2020; Yao et al., 2020)과 더불어 소자의 비대칭성을 해결하기 위한 많은 높은 수준의 완화 기술이 제안되어 왔습니다. 수많은 시뮬레이션 및 실험적 시연이 발표되었지만, 지금까지 어떤 연구도 아날로그 프로세서의 본래 목적인 에너지 효율적인 딥러닝 가속을 달성할 수 있는 솔루션을 제공하지 못했습니다.
기존 기술의 핵심적인 문제는 크로스포인트 요소에 하나씩 또는 행 단위로 직렬로 액세스해야 한다는 점입니다(Prezioso et al., 2015; Yu et al., 2015; Agarwal et al., 2017; Burr et al., 2017; Ambrogio et al., 2018; Li et al., 2018, 2019; Cai et al., 2019; Sebastian et al., 2020). 직렬 연산과 관련된 방법에는 전도도 값을 개별적으로 읽고, 인위적으로 대칭 변조를 강제하는 업데이트 펄스를 엔지니어링하고, 주기적으로 가중치를 운반하거나 리셋하는 방법이 포함됩니다. 또한 일부 접근 방식은 경사도 계산을 디지털 프로세서로 오프로드하는데, 이 경우 아날로그 행렬의 직렬 프로그래밍이 필요할 뿐만 아니라 외부 곱 계산 비용도 부담해야 합니다(Prezioso 등, 2015; Yu 등, 2015; Li 등, 2018, 2019; Cai 등, 2019; Sebastian 등, 2020).
이러한 직렬 루틴으로 N × N 크로스바 배열을 업데이트하려면 최소 N 개 또는 N2 개의 연산이 필요합니다. 실제 배열 크기의 경우, 업데이트 주기에 너무 많은 계산 시간과 에너지가 소요됩니다. 결론적으로, 병렬성을 타협하는 구현의 경우 기존 디지털 프로세서에 비해 계산 처리량과 에너지 효율성 이점이 실제 애플리케이션에서 사라지기 때문에 비대칭성 문제가 해결되었는지 여부는 중요하지 않게 됩니다. 따라서 완전 병렬 연산만 사용하면서 디바이스 비대칭 문제를 해결할 수 있는 방법을 고안하는 것이 시급합니다.

최근 우리 그룹은 비대칭 변조 특성을 가진 비대칭 저항 소자를 기반으로 DNN을 성공적으로 훈련할 수 있는 새로운 완전 병렬 훈련 방법인 Tiki-Taka를 제안했습니다(Gokmen and Haensch, 2020). 이 알고리즘은 비대칭 디바이스 모델로 에뮬레이트된 다양한 네트워크 유형과 크기에 대해 이상적인 디바이스 수준의 분류 정확도를 제공하는 것으로 시뮬레이션을 통해 경험적으로 입증되었습니다(Gokmen and Haensch, 2020). 그러나 제안된 알고리즘 솔루션의 이론적 토대가 부족하고 아날로그 하드웨어를 두 배로 늘리는 데 드는 비용으로 인해 이전에는 Gokmen과 Haensch(2020)에서 설명한 방법이 제한적이었습니다.

이 백서에서는 먼저 디바이스 비대칭성이 SGD 기반 훈련의 근본적인 문제인 이유를 이론적으로 설명합니다. 그리고 고전 역학과의 강력한 유추를 통해 티키-타카 알고리즘이 소자 비대칭의 효과를 통합하여 시스템의 총 에너지(해밀턴)를 최소화한다는 사실을 입증합니다. 본 연구에서는 이 새로운 방법을 확률적 해밀턴 하강(Stochastic Hamiltonian Descent, SHD)으로 공식화하고 완전 병렬 훈련에서 디바이스 비대칭성을 유용한 기능으로 활용할 수 있는 방법을 설명합니다. 고급 물리적 직관을 통해 기존 알고리즘을 개선하고 하드웨어 비용을 50% 절감하여 실질적인 관련성을 향상시킬 수 있었습니다. 다양한 디바이스 제품군에 대한 시뮬레이션 훈련 결과를 사용하여 SHD가 모든 적용 가능한 시나리오에서 SGD 기반 훈련에 비해 더 나은 분류 정확도와 더 빠른 수렴을 제공한다는 결론을 내렸습니다. 이 백서의 내용은 차세대 크로스포인트 요소와 아날로그 컴퓨팅을 위한 특수 알고리즘에 대한 가이드라인을 제공합니다.


### Theory
신경망은 비선형 활성화 함수가 뒤따르는 아핀 변환을 수행하는 여러 행렬(즉, 가중치, W)의 레이어로 구성될 수 있습니다. 훈련(즉, 학습) 프로세스는 주어진 입력에 대한 네트워크 반응이 라벨이 지정된 데이터 세트에 대한 목표 출력을 생성하도록 W를 조정하는 것을 말합니다. 네트워크와 목표 출력 사이의 불일치는 스칼라 오차 함수인 E로 표현되며, 학습 알고리즘은 이를 최소화하려고 노력합니다. 기존 SGD 알고리즘(Cauchy, 1847)의 경우, 각 입력에 대해 샘플링된 오차 함수의 기울기 방향으로 작은 단계(학습 속도 η에 따라 스케일링)를 수행하여 W의 값을 점진적으로 수정합니다. 기울기 계산은 전진 통과, 후진 통과, 업데이트 서브루틴으로 구성된 역전파 알고리즘에 의해 수행됩니다(Rumelhart et al., 1986)(그림 1A). DNN 훈련의 이산적 특성을 연속체 한계에서 분석할 때, W의 시간 진화는 랑그빈 방정식으로 쓸 수 있습니다:

여기서 η는 학습 속도이고 ǫ(t)는 훈련 절차의 내재적 확률성을 설명하는 0 평균을 갖는 변동 항입니다(Feng and Tu, 2023). 이 훈련 과정의 결과로 W는 ∂E ∂W = 0인 최적의 W0 근처로 수렴하지만, ǫ(t)의 존재로 인해 ˙W는 평균적으로 0에 불과합니다. 시각화를 위해 훈련 데이터 세트가 공간에 있는 점들의 클러스터인 경우, W0은 해당 클러스터의 중심이며, 각 개별 점들은 여전히 전체 데이터 세트에서 평균 0이 되는 힘(ǫ(t))을 발휘합니다.
아날로그 크로스바 기반 아키텍처의 경우 선형 행렬 연산은 물리적 디바이스 어레이에서 수행되는 반면, 모든 비선형 연산(예: 활성화 및 오류 함수)은 주변 회로에서 처리됩니다. 디바이스 컨덕턴스의 엄격한 양수 특성으로 인해 한 쌍의 교차점 요소의 미분 컨덕턴스(즉, W ∝ Gmain - Gref)를 통해 각 가중치를 표현해야 합니다.
따라서 정방향 및 역방향 패스에 대한 벡터-행렬 곱셈은 주 배열과 기준 배열을 모두 사용하여 계산됩니다(그림 1A). 반면에 그라데이션 누적 및 업데이트는 참조 배열의 값은 일정하게 유지되는 동안 양방향 컨덕턴스 변화를 사용하여 메인 배열에서만 수행됩니다2.
이 섹션에서는 아날로그 아키텍처를 사용한 DNN 훈련의 기본 역학을 설명하기 위해 가장 단순한 '신경망'으로 간주할 수 있는 단일 파라미터 최적화 문제(선형 회귀)를 연구합니다.

아날로그 구현에서 가중치 업데이트는 크로스포인트 요소의 컨덕턴스 값 변조를 통해 수행되며, 이는 종종 펄스를 통해 적용됩니다. 이러한 펄스는 디바이스 컨덕턴스(1G+,-)에 점진적인 변화를 일으킵니다. 이상적인 디바이스에서 이러한 변조 증분은 그림 1B에 표시된 것처럼 양방향에서 동일한 크기이며 디바이스 컨덕턴스와 무관합니다. 훈련 세트의 서로 다른 입력 샘플은 일반적으로 크기와 부호가 다른 기울기를 생성하므로 훈련 과정의 일련의 변조는 본질적으로 비단조적이라는 점에 유의해야 합니다.
또한 위에서 설명한 것처럼 최적의 컨덕턴스인 G0에 도달하더라도(W0 ∝ G0 - Gref ), 훈련 작업을 계속하면 그림 1C에서와 같이 G0 근처에서 컨덕턴스가 계속 변조됩니다. 결과적으로 G0은 트레이닝 알고리즘 관점에서 디바이스 전도도의 동적 평형점으로 간주할 수 있습니다.

지난 10년간 상당한 기술적 노력에도 불구하고 그림 1B에 표시된 이상적인 특성을 가진 아날로그 저항 소자는 아직 실현되지 않았습니다. 대신, 실제 아날로그 저항 소자는 일반적으로 반대 방향의 단일(즉, 단일 펄스) 변조가 서로 상쇄되지 않는 비대칭 컨덕턴스 변조 특성(즉, 1G+(G) 6= -1G-(G))을 나타냅니다. 그러나 갑작스럽게 리셋되는 상변화 메모리(PCM)와 같은 일부 소자 기술을 제외하고(Burr et al., 2017; Sebastian et al., 2017; Ambrogio et al., 2018), 많은 크로스포인트 요소는 그림 1E와 같이 극한에서 포화 거동을 보이는 매끄럽고 단조로운 비선형 함수로 모델링할 수 있습니다(Kim et al., 2019b, 2020; Yao et al., 2020). 이러한 소자의 경우, 증분 컨덕턴스 변화의 크기가 감소 컨덕턴스의 크기와 동일한 고유한 컨덕턴스 지점인 G대칭이 존재합니다. 결과적으로 훈련 중 G의 시간 변화는 다음과 같이 다시 쓸 수 있습니다:

여기서 κ는 비대칭 계수이고 fhardware는 디바이스 비대칭의 기능적 형태입니다(도출에 대해서는 부록 1.1 참조). 이 식에서 -η ∂E ∂G + ǫ(t)라는 용어는 비대칭 동작과 관련된 변화의 방향이 의도한 변조의 방향과 관계없이 전적으로 f하드웨어에 의해 결정된다는 것을 의미합니다. 그림 1E에 표시된 기하급수적으로 포화되는 디바이스 모델의 경우, fhardware = G - Gsymmetry는 모든 업데이트 이벤트마다 디바이스 전도도를 대칭 지점으로 이동시키는 구성 요소가 있음을 나타냅니다. 이 효과에 대한 간단한 관찰은 이러한 디바이스에 동일한 수의 증분 및 감소 변화가 무작위 순서로 적용될 때 전도도 값이 G대칭 부근으로 수렴한다는 것입니다(Kim et al., 2020). 따라서 이 지점은 동적으로 안정된 유일한 전도도 값이기 때문에 소자의 물리적 평형점으로 볼 수 있습니다. 전자는 전적으로 디바이스에 의존적인 반면 후자는 문제에 의존적이기 때문에 일반적으로 G대칭과 G0 사이에는 아무런 관계가 없다는 것을 인식하는 것이 중요합니다. 결과적으로 비대칭 디바이스의 경우 하드웨어와 소프트웨어의 두 가지 평형이 경쟁하는 시스템을 생성하여 컨덕턴스 값이 Gsymmetry와 G0 사이의 특정 컨덕턴스로 수렴하며, 이 경우 학습 알고리즘과 디바이스 비대칭의 추진력이 균형을 이루게 됩니다(그림 1F). 그림 1C,F에 표시된 예에서 문제의 G0는 비대칭의 효과가 뚜렷하게 나타나는 경우를 묘사하기 위해 의도적으로 Gsymmetry에서 멀리 떨어져 있도록 설계되었습니다. 실제로 최종 수렴 값인 Gfinal과 G0 사이의 불일치는 이상적인 디바이스(그림 1D)와 달리 G대칭에 대한 G0의 상대적 위치에 따라 크게 달라지는 것을 볼 수 있습니다(그림 1G). 이러한 역학 관계에 대한 자세한 내용은 보충 섹션 1.2에서 확인할 수 있습니다. SGD와 달리, 그림 2A에 설명된 새로운 훈련 알고리즘은 업데이트 함수에서 순방향 경로와 오류 역전파를 모두 분리합니다. 이를 위해 각 계층을 표현하기 위해 (단일 쌍이 아닌) 두 개의 배열 쌍, 즉 Amain, Aref , Cmain, Cref가 사용됩니다(Gokmen and Haensch, 2020). 이 표현에서 A = Amain - Aref는 보조 배열을 나타내고 C = Cmain - Cref는 핵심 배열을 나타냅니다. 새로운 훈련 알고리즘은 다음과 같이 작동합니다. 훈련 프로세스가 시작될 때 Aref와 Cref는 각각 Amain,대칭 및 Cmain,대칭으로 초기화됩니다 (이유는 나중에 설명합니다(섹션 M1 참조). 배열 초기화(제로 시프팅)에서 설명하는 방법) 에 따라 Kim 등(2020)의 방법을 따릅니다. 그림 2A에 설명된 것처럼 먼저 배열 쌍 C에 순방향 및 역방향 통과 사이클이 수행되고(단계 I 및 II), Gokmen andVlasov(2016)에서 논의된 병렬 업데이트 방식을 사용하여 Amain(학습률 ηA로 스케일링)에 해당 업데이트가 수행됩니다(단계 III). 즉, 기존 SGD 방식에서는 C에 적용되었을 업데이트가 대신 A에 적용됩니다. 그런 다음 매 τ 주기마다 벡터 u를 사용하여 A에 대해 또 다른 포워드 패스를 수행하여 v = Au를 생성합니다(4단계). 가장 간단한 형태로, u는 하나의 "1"을 제외한 모든 "0"의 벡터일 수 있으며, 그러면 v는 u의 "1" 위치에 해당하는 A의 행과 같아집니다. 마지막으로, 벡터 u와 v는 동일한 병렬 업데이트 방식(학습률 ηc로 조정)으로 Cmain을 업데이트하는 데 사용됩니다(단계 V). 이러한 단계(그림 2A에 표시된 IV와 V)는 기본적으로 A에 저장된 정보를 Cmain에 부분적으로 추가합니다. 알고리즘의 전체 의사 코드는 섹션 M2에서 확인할 수 있습니다. SHD 알고리즘의 의사 코드. 훈련 절차가 끝날 때 C에만 나중에 추론 연산에 사용될 최적화된 네트워크가 포함되어 있습니다(따라서 코어라는 이름이 붙습니다). A는 C가 최적화되면 제로 평균을 갖는 ∂E ∂C 에 대해 계산된 업데이트를 수신하기 때문에 활성 구성 요소인 Amain은 대칭인 Amain을 향해 구동됩니다. 고정 참조 배열인 Aref 를 Amain,대칭에서 초기화하도록 선택하면 이 시점(즉, C가 최적화될 때)에서 A = 0이 되므로 그 대가로 C에 대한 업데이트가 생성되지 않습니다.
