### abstract
프로그래밍 가능한 비휘발성 저항으로 구성된 아날로그 크로스바 어레이는 심층 신경망 훈련의 가속화를 위해 집중적으로 연구되고 있습니다. 그러나 실제 저항 소자의 유비쿼터스 비대칭 전도도 변조는 기존 알고리즘으로 훈련된 네트워크의 분류 성능을 심각하게 저하시킵니다.
여기에서는 먼저 이러한 비호환성의 근본적인 이유를 설명합니다. 그런 다음 비대칭 교차점 요소와 호환되는 새로운 완전 병렬 훈련 알고리즘의 이론적 토대를 설명합니다. 고전 역학과의 강력한 비유를 통해 장치 비대칭성을 아날로그 딥 러닝 프로세서의 유용한 기능으로 활용하는 방법을 설명합니다.
기존의 오차 함수 기울기 방향으로 가중치를 조정하는 대신, 네트워크 파라미터를 프로그래밍하여 소자 비대칭의 효과를 통합한 시스템의 총 에너지(해밀턴)를 성공적으로 최소화할 수 있습니다. 이 기술을 사용하면 쉽게 구할 수 있는 장치 기술을 기반으로 아날로그 딥 러닝 가속기를 즉시 구현할 수 있습니다.

### Introduction
딥러닝은 객체 인식, 자연어 처리, 생물정보학 등의 분야에서 여러 수준의 추상화에서 데이터의 표현을 분류하고 클러스터링함으로써 이점을 얻는 패러다임의 변화를 일으켰습니다(Lecun et al., 2015). 그러나 최첨단 심층 신경망(DNN)을 훈련하기 위한 계산 워크로드는 데이터센터에 막대한 계산 시간과 에너지 비용을 요구합니다(Strubell et al., 2020).
일반적으로 더 큰 데이터 세트로 훈련된 더 큰 신경망이 더 나은 성능을 제공하기 때문에 이러한 추세는 앞으로 더욱 가속화될 것으로 예상됩니다. 그 결과, 딥러닝을 위한 빠르고 에너지 효율적인 솔루션을 제공해야 할 필요성으로 인해 업계와 학계에서 대규모 공동 연구가 진행되고 있습니다(Chen et al., 2016; Jouppi et al., 2017; Rajbhandari et al., 2020). 고도로 최적화된 디지털 애플리케이션별 집적 회로(ASIC) 구현은 계산 집약적인 행렬 연산을 위해 정밀도 감소 연산을 사용하여 DNN 워크로드를 가속화하려고 시도해 왔습니다. 추론 작업의 가속화는 2비트 해상도를 사용하여 달성되었지만(Choi et al., 2019), 학습 작업에는 최소한 하이브리드 8비트 부동 소수점 형식이 필요한 것으로 밝혀졌습니다(Sun et al., 2019). 이는 대규모 네트워크에 여전히 상당한 에너지 소비와 처리 시간을 부과합니다. 따라서 학습 워크로드를 효율적으로 처리할 수 있는 비디지털 접근 방식이 활발히 모색되고 있습니다.

아날로그 저항 크로스바 어레이를 사용한 인메모리 계산 개념은 유망한 대안으로 집중적으로 연구되고 있습니다. 이러한 프레임워크는 옴의 법칙과 키르히호프의 법칙을 사용하여 전체 계산 부하의 ≈ 2/3를 차지하는 병렬 벡터 행렬 곱셈(자세한 내용은 부록 2.1, 2.2 참조)을 수행하도록 처음 설계되었습니다(Steinbuch, 1961). 그러나 업데이트 주기 동안 나머지 ≈ 1/3의 연산도 병렬화하지 않으면 아날로그 어레이가 제공하는 가속 계수는 기존 디지털 프로세서와 비교했을 때 기껏해야 3배에 불과합니다.
나중에 펄스 일치와 디바이스 전도도의 점진적 변화를 사용하여 1등급 외부 제품도 병렬로 달성할 수 있다는 사실이 밝혀졌습니다(Burr et al., 2015; Gokmen and Vlasov, 2016). 이 방법을 사용하면 외부 곱1을 명시적으로 계산하거나 개별 크로스포인트 요소의 값을 읽을 필요 없이 전체 크로스바 어레이를 병렬로 업데이트할 수 있습니다(Gokmen et al., 2017). 그 결과, 확률적 경사 하강(SGD) 알고리즘을 사용하는 DNN 훈련의 모든 기본 요소는 아날로그 크로스바 아키텍처를 사용하여 완전 병렬 방식으로 수행될 수 있습니다.
그러나 이 병렬 업데이트 방식은 크로스포인트 요소의 컨덕턴스 변조 특성에 따라 성능이 크게 영향을 받기 때문에 엄격한 디바이스 요구 사항이 부과됩니다. 특히 비대칭 컨덕턴스 변조 특성(즉, 양의 컨덕턴스와 음의 컨덕턴스 조정이 불일치하는 경우)은 부정확한 기울기 축적을 유발하여 분류 정확도를 저하시키는 것으로 밝혀졌습니다(Yu et al., 2015; Agarwal et al., 2016, 2017; Gokmen and Vlasov, 2016; Gokmen et al., 2017, 2018; Ambrogio et al., 2018). 안타깝게도 현재까지 모든 아날로그 저항 소자는 비대칭적인 특성을 가지고 있어 아날로그 딥러닝 프로세서를 실현하는 데 큰 기술적 장벽이 되고 있습니다.

이상적인 저항 소자를 설계하려는 광범위한 노력(Woo and Yu, 2018; Fuller et al., 2019; Grollier et al., 2020; Yao et al., 2020)과 더불어 소자의 비대칭성을 해결하기 위한 많은 높은 수준의 완화 기술이 제안되어 왔습니다. 수많은 시뮬레이션 및 실험적 시연이 발표되었지만, 지금까지 어떤 연구도 아날로그 프로세서의 본래 목적인 에너지 효율적인 딥러닝 가속을 달성할 수 있는 솔루션을 제공하지 못했습니다.
기존 기술의 핵심적인 문제는 크로스포인트 요소에 하나씩 또는 행 단위로 직렬로 액세스해야 한다는 점입니다(Prezioso et al., 2015; Yu et al., 2015; Agarwal et al., 2017; Burr et al., 2017; Ambrogio et al., 2018; Li et al., 2018, 2019; Cai et al., 2019; Sebastian et al., 2020). 직렬 연산과 관련된 방법에는 전도도 값을 개별적으로 읽고, 인위적으로 대칭 변조를 강제하는 업데이트 펄스를 엔지니어링하고, 주기적으로 가중치를 운반하거나 리셋하는 방법이 포함됩니다. 또한 일부 접근 방식은 경사도 계산을 디지털 프로세서로 오프로드하는데, 이 경우 아날로그 행렬의 직렬 프로그래밍이 필요할 뿐만 아니라 외부 곱 계산 비용도 부담해야 합니다(Prezioso 등, 2015; Yu 등, 2015; Li 등, 2018, 2019; Cai 등, 2019; Sebastian 등, 2020).
이러한 직렬 루틴으로 N × N 크로스바 배열을 업데이트하려면 최소 N 개 또는 N2 개의 연산이 필요합니다. 실제 배열 크기의 경우, 업데이트 주기에 너무 많은 계산 시간과 에너지가 소요됩니다. 결론적으로, 병렬성을 타협하는 구현의 경우 기존 디지털 프로세서에 비해 계산 처리량과 에너지 효율성 이점이 실제 애플리케이션에서 사라지기 때문에 비대칭성 문제가 해결되었는지 여부는 중요하지 않게 됩니다. 따라서 완전 병렬 연산만 사용하면서 디바이스 비대칭 문제를 해결할 수 있는 방법을 고안하는 것이 시급합니다.

최근 우리 그룹은 비대칭 변조 특성을 가진 비대칭 저항 소자를 기반으로 DNN을 성공적으로 훈련할 수 있는 새로운 완전 병렬 훈련 방법인 Tiki-Taka를 제안했습니다(Gokmen and Haensch, 2020). 이 알고리즘은 비대칭 디바이스 모델로 에뮬레이트된 다양한 네트워크 유형과 크기에 대해 이상적인 디바이스 수준의 분류 정확도를 제공하는 것으로 시뮬레이션을 통해 경험적으로 입증되었습니다(Gokmen and Haensch, 2020). 그러나 제안된 알고리즘 솔루션의 이론적 토대가 부족하고 아날로그 하드웨어를 두 배로 늘리는 데 드는 비용으로 인해 이전에는 Gokmen과 Haensch(2020)에서 설명한 방법이 제한적이었습니다.

이 백서에서는 먼저 디바이스 비대칭성이 SGD 기반 훈련의 근본적인 문제인 이유를 이론적으로 설명합니다. 그리고 고전 역학과의 강력한 유추를 통해 티키-타카 알고리즘이 소자 비대칭의 효과를 통합하여 시스템의 총 에너지(해밀턴)를 최소화한다는 사실을 입증합니다. 본 연구에서는 이 새로운 방법을 확률적 해밀턴 하강(Stochastic Hamiltonian Descent, SHD)으로 공식화하고 완전 병렬 훈련에서 디바이스 비대칭성을 유용한 기능으로 활용할 수 있는 방법을 설명합니다. 고급 물리적 직관을 통해 기존 알고리즘을 개선하고 하드웨어 비용을 50% 절감하여 실질적인 관련성을 향상시킬 수 있었습니다. 다양한 디바이스 제품군에 대한 시뮬레이션 훈련 결과를 사용하여 SHD가 모든 적용 가능한 시나리오에서 SGD 기반 훈련에 비해 더 나은 분류 정확도와 더 빠른 수렴을 제공한다는 결론을 내렸습니다. 이 백서의 내용은 차세대 크로스포인트 요소와 아날로그 컴퓨팅을 위한 특수 알고리즘에 대한 가이드라인을 제공합니다.


### Theory
신경망은 비선형 활성화 함수가 뒤따르는 아핀 변환을 수행하는 여러 행렬(즉, 가중치, W)의 레이어로 구성될 수 있습니다. 훈련(즉, 학습) 프로세스는 주어진 입력에 대한 네트워크 반응이 라벨이 지정된 데이터 세트에 대한 목표 출력을 생성하도록 W를 조정하는 것을 말합니다. 네트워크와 목표 출력 사이의 불일치는 스칼라 오차 함수인 E로 표현되며, 학습 알고리즘은 이를 최소화하려고 노력합니다. 기존 SGD 알고리즘(Cauchy, 1847)의 경우, 각 입력에 대해 샘플링된 오차 함수의 기울기 방향으로 작은 단계(학습 속도 η에 따라 스케일링)를 수행하여 W의 값을 점진적으로 수정합니다. 기울기 계산은 전진 통과, 후진 통과, 업데이트 서브루틴으로 구성된 역전파 알고리즘에 의해 수행됩니다(Rumelhart et al., 1986)(그림 1A). DNN 훈련의 이산적 특성을 연속체 한계에서 분석할 때, W의 시간 진화는 랑그빈 방정식으로 쓸 수 있습니다:

여기서 η는 학습 속도이고 ǫ(t)는 훈련 절차의 내재적 확률성을 설명하는 0 평균을 갖는 변동 항입니다(Feng and Tu, 2023). 이 훈련 과정의 결과로 W는 ∂E ∂W = 0인 최적의 W0 근처로 수렴하지만, ǫ(t)의 존재로 인해 ˙W는 평균적으로 0에 불과합니다. 시각화를 위해 훈련 데이터 세트가 공간에 있는 점들의 클러스터인 경우, W0은 해당 클러스터의 중심이며, 각 개별 점들은 여전히 전체 데이터 세트에서 평균 0이 되는 힘(ǫ(t))을 발휘합니다.
아날로그 크로스바 기반 아키텍처의 경우 선형 행렬 연산은 물리적 디바이스 어레이에서 수행되는 반면, 모든 비선형 연산(예: 활성화 및 오류 함수)은 주변 회로에서 처리됩니다. 디바이스 컨덕턴스의 엄격한 양수 특성으로 인해 한 쌍의 교차점 요소의 미분 컨덕턴스(즉, W ∝ Gmain - Gref)를 통해 각 가중치를 표현해야 합니다.
따라서 정방향 및 역방향 패스에 대한 벡터-행렬 곱셈은 주 배열과 기준 배열을 모두 사용하여 계산됩니다(그림 1A). 반면에 그라데이션 누적 및 업데이트는 참조 배열의 값은 일정하게 유지되는 동안 양방향 컨덕턴스 변화를 사용하여 메인 배열에서만 수행됩니다2.
이 섹션에서는 아날로그 아키텍처를 사용한 DNN 훈련의 기본 역학을 설명하기 위해 가장 단순한 '신경망'으로 간주할 수 있는 단일 파라미터 최적화 문제(선형 회귀)를 연구합니다.

아날로그 구현에서 가중치 업데이트는 크로스포인트 요소의 컨덕턴스 값 변조를 통해 수행되며, 이는 종종 펄스를 통해 적용됩니다. 이러한 펄스는 디바이스 컨덕턴스(1G+,-)에 점진적인 변화를 일으킵니다. 이상적인 디바이스에서 이러한 변조 증분은 그림 1B에 표시된 것처럼 양방향에서 동일한 크기이며 디바이스 컨덕턴스와 무관합니다. 훈련 세트의 서로 다른 입력 샘플은 일반적으로 크기와 부호가 다른 기울기를 생성하므로 훈련 과정의 일련의 변조는 본질적으로 비단조적이라는 점에 유의해야 합니다.
또한 위에서 설명한 것처럼 최적의 컨덕턴스인 G0에 도달하더라도(W0 ∝ G0 - Gref ), 훈련 작업을 계속하면 그림 1C에서와 같이 G0 근처에서 컨덕턴스가 계속 변조됩니다. 결과적으로 G0은 트레이닝 알고리즘 관점에서 디바이스 전도도의 동적 평형점으로 간주할 수 있습니다.

지난 10년간 상당한 기술적 노력에도 불구하고 그림 1B에 표시된 이상적인 특성을 가진 아날로그 저항 소자는 아직 실현되지 않았습니다. 대신, 실제 아날로그 저항 소자는 일반적으로 반대 방향의 단일(즉, 단일 펄스) 변조가 서로 상쇄되지 않는 비대칭 컨덕턴스 변조 특성(즉, 1G+(G) 6= -1G-(G))을 나타냅니다. 그러나 갑작스럽게 리셋되는 상변화 메모리(PCM)와 같은 일부 소자 기술을 제외하고(Burr et al., 2017; Sebastian et al., 2017; Ambrogio et al., 2018), 많은 크로스포인트 요소는 그림 1E와 같이 극한에서 포화 거동을 보이는 매끄럽고 단조로운 비선형 함수로 모델링할 수 있습니다(Kim et al., 2019b, 2020; Yao et al., 2020). 이러한 소자의 경우, 증분 컨덕턴스 변화의 크기가 감소 컨덕턴스의 크기와 동일한 고유한 컨덕턴스 지점인 G대칭이 존재합니다. 결과적으로 훈련 중 G의 시간 변화는 다음과 같이 다시 쓸 수 있습니다:

여기서 κ는 비대칭 계수이고 fhardware는 디바이스 비대칭의 기능적 형태입니다(도출에 대해서는 부록 1.1 참조). 이 식에서 -η ∂E ∂G + ǫ(t)라는 용어는 비대칭 동작과 관련된 변화의 방향이 의도한 변조의 방향과 관계없이 전적으로 f하드웨어에 의해 결정된다는 것을 의미합니다. 그림 1E에 표시된 기하급수적으로 포화되는 디바이스 모델의 경우, fhardware = G - Gsymmetry는 모든 업데이트 이벤트마다 디바이스 전도도를 대칭 지점으로 이동시키는 구성 요소가 있음을 나타냅니다. 이 효과에 대한 간단한 관찰은 이러한 디바이스에 동일한 수의 증분 및 감소 변화가 무작위 순서로 적용될 때 전도도 값이 G대칭 부근으로 수렴한다는 것입니다(Kim et al., 2020). 따라서 이 지점은 동적으로 안정된 유일한 전도도 값이기 때문에 소자의 물리적 평형점으로 볼 수 있습니다. 전자는 전적으로 디바이스에 의존적인 반면 후자는 문제에 의존적이기 때문에 일반적으로 G대칭과 G0 사이에는 아무런 관계가 없다는 것을 인식하는 것이 중요합니다. 결과적으로 비대칭 디바이스의 경우 하드웨어와 소프트웨어의 두 가지 평형이 경쟁하는 시스템을 생성하여 컨덕턴스 값이 Gsymmetry와 G0 사이의 특정 컨덕턴스로 수렴하며, 이 경우 학습 알고리즘과 디바이스 비대칭의 추진력이 균형을 이루게 됩니다(그림 1F).

그림 1C,F에 표시된 예에서 문제의 G0는 비대칭의 효과가 뚜렷하게 나타나는 경우를 묘사하기 위해 의도적으로 Gsymmetry에서 멀리 떨어져 있도록 설계되었습니다. 실제로 최종 수렴 값인 Gfinal과 G0 사이의 불일치는 이상적인 디바이스(그림 1D)와 달리 G대칭에 대한 G0의 상대적 위치에 따라 크게 달라지는 것을 볼 수 있습니다(그림 1G). 이러한 역학 관계에 대한 자세한 내용은 보충 섹션 1.2에서 확인할 수 있습니다. SGD와 달리, 그림 2A에 설명된 새로운 훈련 알고리즘은 업데이트 함수에서 순방향 경로와 오류 역전파를 모두 분리합니다. 이를 위해 각 계층을 표현하기 위해 (단일 쌍이 아닌) 두 개의 배열 쌍, 즉 Amain, Aref , Cmain, Cref가 사용됩니다(Gokmen and Haensch, 2020). 이 표현에서 A = Amain - Aref는 보조 배열을 나타내고 C = Cmain - Cref는 핵심 배열을 나타냅니다.
새로운 훈련 알고리즘은 다음과 같이 작동합니다. 훈련 프로세스가 시작될 때 Aref와 Cref는 각각 Amain,대칭 및 Cmain,대칭으로 초기화됩니다 (이유는 나중에 설명합니다(섹션 M1 참조). 배열 초기화(제로 시프팅)에서 설명하는 방법) 에 따라 Kim 등(2020)의 방법을 따릅니다. 그림 2A에 설명된 것처럼 먼저 배열 쌍 C에 순방향 및 역방향 통과 사이클이 수행되고(단계 I 및 II), Gokmen andVlasov(2016)에서 논의된 병렬 업데이트 방식을 사용하여 Amain(학습률 ηA로 스케일링)에 해당 업데이트가 수행됩니다(단계 III). 즉, 기존 SGD 방식에서는 C에 적용되었을 업데이트가 대신 A에 적용됩니다. 그런 다음 매 τ 주기마다 벡터 u를 사용하여 A에 대해 또 다른 포워드 패스를 수행하여 v = Au를 생성합니다(4단계). 가장 간단한 형태로, u는 하나의 "1"을 제외한 모든 "0"의 벡터일 수 있으며, 그러면 v는 u의 "1" 위치에 해당하는 A의 행과 같아집니다. 마지막으로, 벡터 u와 v는 동일한 병렬 업데이트 방식(학습률 ηc로 조정)으로 Cmain을 업데이트하는 데 사용됩니다(단계 V).
이러한 단계(그림 2A에 표시된 IV와 V)는 기본적으로 A에 저장된 정보를 Cmain에 부분적으로 추가합니다. 알고리즘의 전체 의사 코드는 섹션 M2에서 확인할 수 있습니다. SHD 알고리즘의 의사 코드. 훈련 절차가 끝날 때 C에만 나중에 추론 연산에 사용될 최적화된 네트워크가 포함되어 있습니다(따라서 코어라는 이름이 붙습니다). A는 C가 최적화되면 제로 평균을 갖는 ∂E ∂C 에 대해 계산된 업데이트를 수신하기 때문에 활성 구성 요소인 Amain은 대칭인 Amain을 향해 구동됩니다. 고정 참조 배열인 Aref 를 Amain,대칭에서 초기화하도록 선택하면 이 시점(즉, C가 최적화될 때)에서 A = 0이 되므로 그 대가로 C에 대한 업데이트가 생성되지 않습니다.

위에서 u 벡터를 선택하면 4단계와 5단계를 수행할 때마다 u 벡터의 "1"의 위치가 주기적으로 변경되지만, 일반적으로는 모든 직교 u 벡터 집합을 이 목적으로 사용할 수 있습니다(Gokmen and Haensch, 2020). 이러한 단계를 가중치 운반과 혼동해서는 안 된다는 점을 강조하는데, ηC << 1이 되면 C는 A 방향으로 극히 일부분만 업데이트되고 A에 저장된 정보는 외부에서 지워지지 않으므로(즉, A는 리셋되지 않습니다) 이러한 단계를 가중치 운반(Agarwal et al., 2017; Ambrogio et al., 2018)과 혼동하면 안 된다는 점을 강조합니다.
대신, A와 C는 서로의 값에 의해 변경이 결정되기 때문에 결합된 동역학 시스템을 형성합니다. 또한, 그림 2에 표시된 알고리즘은 완전 병렬 연산으로만 구성되어 있다는 점을 인식하는 것이 중요합니다. 단계 I 및 II(C에 대한 순방향 및 역방향 전달)와 마찬가지로 단계 IV는 옴의 법칙과 키르히호프의 법칙에 의해 수행되는 또 다른 행렬-벡터 곱셈입니다.
반면에 업데이트 단계 III과 V는 확률적 업데이트 방식에 의해 수행됩니다(Gokmen and Vlasov, 2016). 이 업데이트 방식은 명시적으로 외부 곱(x × δ 및 u × v)을 계산하지 않고 통계적 방법을 사용하여 모든 가중치를 해당 외부 곱에 비례하여 병렬로 수정합니다. 따라서 학습 작업의 어느 시점에서도 직렬 연산이 필요하지 않으므로 딥 러닝 계산에서 높은 처리량과 에너지 효율성이라는 이점을 누릴 수 있습니다. 위에서 연구한 동일한 선형 회귀 문제의 경우, 그림 2A에 주어진 이산 시간 업데이트 규칙은 그림 2C와 같이 하위 시스템 A와 C의 시간 진화를 설명하는 연속체 한계에서 한 쌍의 미분 방정식으로 다시 작성할 수 있습니다(그림 2C):


결합 시스템에 대한 이 설명은 감쇠 고조파 발진기의 동작을 지배하는 방정식과 동일한 배열을 가지고 있음을 알 수 있습니다(그림 2B,D). 이 비유에서 하위 시스템A는 속도 ν에 대응하고 하위 시스템C는 위치 x에 매핑되어 최적화 문제3의 스칼라 오차 함수인 (C - C0)2가 물리적 프레임워크의 스칼라 위치 에너지인 1 2 kspring (x - x0)2에 매핑될 수 있습니다. 또한 비대칭 디바이스를 사용한 구현의 경우, 하드웨어로 인한 컨덕턴스 변조에 대한 효과를 반영하기 위해 미분 방정식에 추가 힘 항인 F하드웨어를 포함시켜야 합니다. 앞서 설명한 대로 그림 1E에 표시된 디바이스 모델의 경우 이 항은 Amain - Amain,대칭에 비례합니다. Aref = Amain,대칭이라고 가정하면(이 가정은 나중에 설명합니다) Fhardware를 Amain-Aref 의 함수로 다시 쓸 수 있으며, 이는 속도(ν ∝ A = Amain - Aref )에 선형 비례하는 가변(그러나 엄밀히 음수가 아닌) 항력 계수 kdrag 와 유사한 항력인 Fdrag 와 비슷해집니다. 일반적으로 F하드웨어 항은 전도도 변조 특성이 다른 디바이스의 경우 다양한 기능적 형태를 가질 수 있지만 이상적인 디바이스에는 전혀 존재하지 않습니다. 물리적 유추를 단순화하기 위해 서브시스템 C의 비대칭 효과를 무시하고 방정식 4 대신 그림 2C에 표시된 방정식을 도출했습니다. 이 결정은 섹션 토론에서 정당화될 것입니다. 무손실 고조파 발진기의 동작과 유사하게, 이 수정된 최적화 문제의 정상 상태 솔루션은 이상적인 장치(즉, F하드웨어 = 0)에서 진동 동작을 갖습니다(그림 2E). 이 결과는 소산 메커니즘이 없는 경우 시스템의 총 에너지를 최소화할 수 없고(일정함) 전위와 운동 성분 사이에서만 지속적으로 변환될 수 있기 때문에 예상되는 결과입니다. 반면 비대칭 소자의 경우 소산력 항 F하드웨어는 시스템의 모든 에너지를 서서히 소멸시켜 A ∝ ν은 0으로 수렴하고(운동 → 0), C ∝ x는 C0 ∝ x0으로 수렴합니다(전위 → 0). 이러한 관찰을 바탕으로 시스템의 총 에너지(해밀턴)를 감소시키는 방향으로 시스템 파라미터의 진화를 강조하기 위해 새로운 학습 알고리즘의 이름을 확률적 해밀턴 하강(SHD)으로 변경했습니다. 이러한 역학은 이상적인 무손실 시스템에서 관찰되는 타원형 궤적(그림 2F)과 달리 비대칭 장치를 사용한 최적화 프로세스의 감쇠 진동을 나타내는 나선형 경로(그림 2E)를 생성하는 A의 시간 진화와 C의 시간 진화를 플롯하여 시각화할 수 있습니다.

소산적 특성의 필요성을 확립한 다음, 여기서는 디바이스 비대칭성이 이러한 동작을 제공하는 조건을 분석합니다. 역학에서 힘이 소산성으로 간주되려면 속도(즉, 힘)와의 곱이 음수여야 한다는 것은 잘 알려져 있습니다(그렇지 않으면 시스템에 에너지가 주입되는 것을 의미하기 때문입니다). 즉, 하드웨어로 인한 힘 항 F하드웨어 = -κAηA ∂E ∂C + ǫ(t) (Amain -Amain,대칭)와 속도 ν = Amain - Aref 는 항상 반대 부호를 가져야 합니다. 또한 정상 상태 해석에서 시스템이 최소 위치 에너지(x = x0)를 갖는 지점에서 정지 상태(ν = 0)가 되려면 순 힘(F = 0)이 없어야 합니다. 이 두 가지 인수는 SHD 알고리즘이 제대로 작동하려면 Aref가 대칭인 Amain으로 설정되어야 함을 나타냅니다. 크로스포인트 요소가 비대칭 장치로 구현되고(SGD 요구 사항과 반대) 각 장치에 대칭점이 존재하는 한, 변조 특성의 모양은 SHD 알고리즘을 사용한 성공적인 DNN 훈련에 중요하지 않습니다. 중요한 것은 대칭형 디바이스에 대한 기술적으로 실행 가능한 솔루션은 수십 년 동안의 연구에도 불구하고 아직 발견되지 않았지만, 앞서 언급한 특성을 만족하는 비대칭형 디바이스는 풍부하다는 점입니다.

주목해야 할 중요한 측면은 SGD 알고리즘과 SHD 알고리즘은 근본적으로 완전히 다른 역학 관계에 의해 지배되는 별개의 방법이라는 점입니다. SGD 알고리즘은 디바이스 비대칭의 영향을 무시한 채 시스템 파라미터를 최적화하려고 시도하므로 잘못된 에너지 함수의 최소값으로 수렴합니다. 반면, SHD 기반 훈련의 시스템 변수는 일반적으로 오차 함수 기울기 방향으로 진화하지 않고 하드웨어 유도 조건을 통합한 총 에너지를 최소화하도록 조정됩니다.
이러한 속성의 가장 분명한 현상은 학습이 최적 지점(즉, 매우 운이 좋은 추측 시나리오)에서 초기화될 때 관찰할 수 있는데, 모든 "학습" 알고리즘은 최소한 이 최적 상태를 유지할 수 있어야 하기 때문입니다. 기존 SGD의 경우, W = W0일 때 네트워크에 적용된 제로-평균 업데이트는 위에 표시된 바와 같이 W를 W0에서 W비대칭으로 멀어지게 합니다. 반면, SHD 방식에서는 A = 0, C = C0일 때 A에 적용된 제로-평균 업데이트는 A = 0에 대해 이미 대칭인 Amain에 있기 때문에 부작용이 없습니다. 결과적으로 ˙C = A = 0이므로 C에도 업데이트가 적용되지 않습니다.
따라서 SGD는 처음부터 솔루션을 정확하게 추측하더라도 비대칭 장치와 근본적으로 호환되지 않는 반면, SHD는 이러한 문제가 발생하지 않는다는 것이 분명해졌습니다. SGD에 대해 제안된 명제는 평형 전파(Scellier and Bengio, 2017) 및 심층 볼츠만 기계(Salakhutdinov and Hinton, 2009)와 같은 다른 크로스바 호환 훈련 방법으로 더 일반화할 수 있으며, 이 논문에서 논의한 접근 방식에 따라 비대칭 장치에 사용하도록 조정할 수도 있습니다.

마지막으로, 대규모 신경망은 여기서 분석한 문제와 관련하여 훨씬 더 복잡한 시스템이라는 점을 이해해 주시기 바랍니다. 마찬가지로, 다양한 아날로그 디바이스는 광범위한 컨덕턴스 변조 동작을 보일 뿐만 아니라 아날로그 노이즈, 불완전한 유지, 제한된 내구성과 같은 다른 비이상적 특성을 지니고 있습니다. 하지만 여기서 소개하는 이론은 이에 대한 직관적인 설명을 제공합니다: (1) 디바이스 비대칭성이 SGD 기반 훈련과 근본적으로 호환되지 않는 이유, (2) 완전 병렬 연산만 사용하면서 정확한 최적화를 보장하는 방법. 우리는 분류 문제에 대한 최적이 정상 상태에서도 안정적인 솔루션이 아닌 경쟁 평형의 맥락에서 SGD 내의 비대칭성 관련 문제를 분석해야 한다는 결론을 내렸습니다. 이러한 간단한 안정성 분석 외에도 최적화가 아닌 하드웨어 효과를 포함하도록 최적화 환경을 수정하는 통찰력을 통해 향후 최적 제어 이론의 고급 개념을 사용하여 다른 완전 병렬 솔루션을 설계할 수 있습니다. 결과적으로 이러한 병렬 방식을 통해 아날로그 프로세서는 기존 디지털 프로세서에 비해 높은 연산 처리량과 에너지 효율성 이점을 제공할 수 있습니다.


### Experimental Demonstration
위에서 이론화한 SHD 역학을 검증하기 위해 세바스찬 외(2017)에서 보고한 금속 산화물 기반 전기화학 소자를 사용하여 SHD 알고리즘의 실험적 데모를 수행했습니다(그림 3A). 이 소자는 3단자4, 전압 제어 크로스포인트 소자로, 컴플라이언스 회로나 직렬 액세스 장치가 없습니다. 디바이스 중 하나에 대해 얻은 변조 특성은 그림 3B에 표시되어 있으며, 잘 정의된 대칭점에서 "교차 검" 동작이 관찰됩니다.
SHD 기반 훈련의 본질을 포착하기 위해 합성 데이터 세트 x1,2와 y = G01x1 + G02x2 + γ 형태로 생성된 2파라미터 최적화 문제를 선택했습니다. 여기서 G01,2는 검색된 미지수이고 γ는 가우스 노이즈입니다. 순방향 및 역방향 통과 주기 동안 (훈련 세트의) 입력 값은 서로 다른 전압 레벨로 표현되고 라인 전류를 측정하여 출력 결과를 얻었습니다. 실제 구현에서는 진폭이 아닌 다른 펄스 폭으로 입력 값을 표현하는 것이 정확한 벡터-행렬 곱셈을 위해 크로스포인트 요소의 비선형 전도도의 영향을 피하는 데 유리할 수 있다는 점에 유의해야 합니다.
업데이트 벡터인 x와 δ를 생성한 후, 어레이는 Gokmen과 Vlasov(2016)에서 설명한 대로 반편향 전압 방식을 사용한 확률적 업데이트를 사용하여 병렬로 프로그래밍됩니다. 따라서 외부 곱을 명시적으로 계산하거나 어떤 지점에서든 디바이스에 직렬로 액세스하지 않았습니다(그림 3C). SHD 알고리즘을 사용한 어레이 훈련 결과는 그림 3D에 나와 있습니다. A1과 A2는 모두 0에 수렴하는 반면, C1과 C2는 최적값에 성공적으로 수렴하는 것을 볼 수 있습니다. 또한 두 변수 모두에서 독특한 나선형 거동(즉, 감쇠 진동)이 관찰되어 소산성 기계 시스템과 유사한 동역학을 보였습니다.
훈련 작업의 성공 여부는 디바이스의 대칭점의 안정성에 크게 좌우된다는 사실을 발견했습니다. 앞서 설명한 것처럼, 장치의 대칭점과 기준점(훈련 시작 시 대칭점으로 초기화됨) 사이의 불일치는 정상 상태 속도가 0이 아님을 나타냅니다. 따라서 미래의 크로스포인트 디바이스 기술은 훈련 작업 전반에 걸쳐 최소한 준정적 상태를 유지하는 잘 정의된 대칭점을 보여야 합니다.

### DISCUSSION AND SIMULATED TRAINING RESULTS
이 섹션에서는 먼저 결합 시스템의 이론적 분석에서 얻은 직관을 사용해 배열이 4개가 아닌 3개인 SHD 알고리즘을 구현하는 방법에 대해 설명합니다. 그런 다음 다양한 비대칭 특성에 대한 대규모 신경망에 대한 시뮬레이션 결과를 제공하여 SGD 기반 훈련과 비교하여 우리의 방법을 벤치마킹합니다. 무작위 순서로 m + n 개의 증분 및 감소 변화 시퀀스를 고려할 때 대칭 장치에 대해 얻은 순 변조는 평균 m입니다. 반면 비대칭 디바이스의 경우 전도도 값은 (m 또는 초기 전도도에 관계없이) n이 증가하면 결국 대칭 지점에 수렴한다는 것을 위에서 살펴보았습니다. 

훈련 데이터에 존재하는 통계적 변동이 증가하면(업데이트에 더 많은 방향 변화를 일으킴) 디바이스 비대칭의 효과가 더욱 두드러져 기존 SGD로 훈련된 네트워크의 분류 정확도가 더 크게 저하되는 것을 확인할 수 있습니다(보충 그림 S1 참조).그러나 이러한 동작은 지속적인 부호 정보인 m+2n 을 가진 신호만 통과하는 비선형 필터링으로 볼 수도 있습니다. 실제로 SHD 알고리즘은 핵심 어레이인 C를 훈련하는 데 사용되는 기울기 정보를 필터링하는 보조 어레이인 A 내에서 이 속성을 활용합니다. 결과적으로 C는 당면한 문제의 오류 함수를 최소화하는 높은 신뢰 수준을 가진 방향으로만 더 적은 빈도로 업데이트됩니다. 이 설명의 직접적인 의미는 C의 비대칭 변조 동작이 업데이트 신호에 통계적 변동이 적기 때문에 성공적인 최적화를 위해 A보다 훨씬 덜 중요하다는 것입니다(보충 그림 S2 참조). 따라서 Cmain의 대칭점 정보도 중요하지 않습니다.
이러한 결과와 직관을 바탕으로 원래 알고리즘을 수정하여 Cref를 버리고 Aref(대칭인 Amain으로 설정)를 차동 판독을 위한 공통 기준 배열로 사용했습니다. 이 수정으로 SHD 구현의 하드웨어 비용이 50% 감소하여 실용성이 크게 향상되었습니다. 비대칭을 소산 메커니즘으로 설명한 것은 비대칭이 SHD 프레임워크 내에서 수렴을 위해 필요하고 유용한 장치 속성이라는 것을 나타냅니다(그림 2E). 그러나 이 주장이 실제 크기의 애플리케이션에서 디바이스 비대칭의 크기에 따라 수렴 속도가 결정된다는 것을 의미하지는 않습니다.
위에서 고려한 단일 매개변수 회귀 문제와는 달리, DNN 학습을 위한 탐색 공간은 엄청나게 크기 때문에 데이터 세트의 여러 반복에 걸쳐 최적화가 이루어집니다. 그 대신, 시스템 진화의 균형을 맞추는 데 필요한 비대칭 수준(즉, 댐핑)은 매우 작으며 실제 비대칭 수준에서 쉽게 달성할 수 있습니다. 이러한 주장을 증명하기 위해, 그림 4에서는 SGD와 SHD 알고리즘으로 훈련된 비대칭 수준이 증가하는 장치 모델을 사용하여 장단기 메모리(LSTM) 네트워크에 대한 시뮬레이션 결과를 보여줍니다. 이 네트워크는 주어진 텍스트 문자열의 다음 문자를 예측하기 위해 레오 톨스토이의 소설 '전쟁과 평화'로 훈련되었습니다(Karpathy et al., 2015).

참고로, 동일한 네트워크를 32비트 디지털 부동소수점 아키텍처로 훈련하면 교차 엔트로피 수준이 1.33이 됩니다(보충 그림 S6에 표시된 전체 학습 곡선). 특히 LSTM은 디바이스 비대칭성에 특히 취약한 것으로 알려져 있기 때문에 이 네트워크를 선택했습니다(Gokmen et al., 2018). 그림 4의 삽입물은 각 비대칭 레벨을 대표하는 평균 컨덕턴스 변조 특성을 보여줍니다. 시뮬레이션에는 Gokmen과 Vlasov(2016)에서 수행한 작업과 유사하게 디바이스 간 변동, 사이클 간 변동, 아날로그 판독 노이즈, 확률적 업데이트가 추가로 포함되었습니다. 학습 곡선은 분류 모델의 성능을 측정하는 교차 엔트로피 오차의 진화를 훈련 기간에 따라 보여줍니다. 먼저, 그림 4A는 SGD로 훈련된 최소 비대칭 장치(파란색 트레이스)의 경우에도 분류 성능의 페널티가 이미 심각하다는 것을 보여줍니다. 이 결과는 SGD로 정확하게 훈련할 수 있을 만큼 대칭적인 디바이스를 설계하는 것이 얼마나 어려운지를 다시 한 번 보여줍니다. 반면, SHD(그림 4B)의 경우, 예상대로 완벽하게 대칭인 디바이스(검은색 트레이스)를 제외하고는 표시된 모든 디바이스가 성공적으로 훈련되었습니다(급격한 변조 특성을 가진 디바이스에 대해서는 보충 그림 S3 참조). 또한 그림 4B는 SHD가 SGD로 훈련한 완전 대칭 디바이스보다 더 높은 정확도와 더 빠른 수렴으로 훈련 결과를 제공할 수 있음을 보여줍니다. 결과적으로 아날로그 딥러닝 아키텍처에서는 일반적으로 SHD가 SGD보다 우수하다는 결론을 내릴 수 있습니다. 마지막으로, SHD를 아날로그 컴퓨팅의 맥락에서 구체적으로 설명했지만, 시뮬레이션 비대칭을 사용하는 기존 프로세서에서도 잠재적으로 유용할 수 있습니다. 위에서 설명한 필터링 역학 덕분에 SHD는 통계적 지속성이 높은 방향으로 핵심 구성 요소를 선택적으로 안내할 수 있습니다. 따라서 전체 메모리와 연산 횟수를 늘리는 대신 SHD는 더 빠른 수렴, 더 나은 분류 정확도 및/또는 우수한 일반화 성능을 제공함으로써 기존 학습 알고리즘보다 더 나은 성능을 발휘할 수 있습니다.

